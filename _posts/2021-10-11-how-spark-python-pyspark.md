---
layout: post
title: "Comment Spark peut √™tre lanc√© √† travers Python ? üí´"
author: "Omar LARAQUI"
categories: journal
tags: [bigdata,spark,pyspark,py4j]
image: pyspark.jpg
---

## Qu\'est ce que PySpark

PySpark est une API Python pour Spark publi√©e par la communaut√© Apache Spark pour prendre en charge Python avec Spark. En utilisant PySpark, on peut facilement int√©grer et travailler avec des RDD dans le langage de programmation Python. Il existe de nombreuses fonctionnalit√©s qui font de PySpark un cadre aussi incroyable lorsqu\'il s\'agit de travailler avec d\'√©normes ensembles de donn√©es. Que ce soit pour effectuer des calculs sur de grands ensembles de donn√©es ou simplement pour les analyser.

## Pourquoi Python?

Les data engineers se tournent de plus en plus vers PySpark pour la simplicit√© du langage Python mais ce n\'est pas la seule raison. En effet, la majorit√© des data scientists utilisent Python comme premier langage de programmation. Ce passage √† l\'√©chelle sur les donn√©es qu\'ils utilisent, pr√©parent, formatent et sauvegardent demande l\'utilisation du calcul distribu√© propos√© notamment par PySpark. Sans oublier que Spark comporte une brique pour entrainer des mod√®les Machine Learning appel√©e MLlib.

## Comment Python peut-il lancer et g√©rer des jobs Spark?

En √©tant une librairie tournant sur une JVM (Java Virtual Machine), on peut facilement se demander comment est ce possible de le lancer des jobs Spark via Python. Ce dernier √©tant un langage interpr√©t√©.

En consultant le code source de PySpark, on retrouve le fichier **java_gateway.py** et c\'est ce fichier la qui nous montera comment Python peut tourner Spark, ou plut√¥t interagir avec la JVM du noeud maitre de Spark.

Concr√®tement, toute la partie traitement des donn√©es est g√©r√©e par Python et toute la partie persistance et √©change de donn√©es entre les √©x√©cuteurs (worker nodes) et le maitre (master node) sont g√©r√©s par la JVM des process Spark.

En initialisant la session Spark, on cr√©e un contexte Spark. Ce contexte est utilis√© pour cr√©er une socket TCP avec un process Py4J.

> Py4J est une biblioth√®que Java int√©gr√©e √† PySpark et permettant √† Python de s\'interfacer dynamiquement avec des objets JVM.

Le process Py4J √©tant responsable de communiquer les diff√©rentes transformations et actions appliqu√©es √† des RDDs (ou √† un plus haut niveau des DataFrames ou DataSets) en les s√©rialisant. La s√©rialisation passe par le biais du fichier **cloudpickle.py** utilisant la biblioth√®que pickle de Python. Il les envoi alors vers le process Py4J qui les d√©s√©rialise puis les envoie √† son tour au noeud maitre du job Spark en tant qu\'objets PythonRDD, le plan d\'√©x√©cution Spark est donc modifi√© pour en tenir compte. Il est ex√©cut√© sur les worker nodes en cas d\'op√©rations de transformation.

![how_python_runs_spark](assets/img/how-python-spark/pyspark_architecture_overview.png "Comment Python peut-il lancer Spark?")

## Et en cas d\'une op√©ration de type action?

Une op√©ration de type action sur un PythonRDD d√©clenche en plus le chemin inverse c\'est √† dire l\'envoi des r√©sultats sous forme de PythonRDD au noeud maitre qui les envoie √† son tour au process Py4J. Ce dernier s√©rialise l\'objet PythonRDD au sein de sa JVM et l\'envoie au driver PySpark via la m√™me socket TCP cr√©√©e pr√©c√©demment. A son arriv√©e, il est d√©s√©rialis√© et redevient un PythonRDD.

## Aurons nous les m√™mes performances que de lancer Spark sur Scala ou Java?

Du moment que toute la partie persistance des donn√©es est g√©r√©e sur la JVM des noeuds maitre et executeurs, on sera sur des performances √©gales √† des runs sur Scala ou Java. La diff√©rence sur Python est principalement due au travail effectu√© par le process de s√©rialisation entre le driver PySpark et le process Py4J. Tout d√©pendra donc de la taille des donn√©es que l\'on est entrain de s√©rialiser ou d√©s√©rialiser. N√©anmoins, avec la puissance de calcul en augmentation continue, on peut juger que cette diff√©rence de l\'ordre du millisecondes pr√®s peut √™tre parfois n√©gligeable devant la possibilit√© d\'utiliser un langage aussi simple et intuitif que Python pour lancer des jobs Spark. üôÇ

Pour pousser vos recherches sur les sujets abord√©s lors de ce blog, je vous invite √† consulter:

- [La page officielle de Py4J](https://www.py4j.org/) pour comprendre comment un interpr√©teur Python √©change avec une JVM
- [La biblioth√®que pickle](https://docs.python.org/3/library/pickle.html) de Python pour la serialisation et deserialisation d\'objets
- [Le code source officiel de PySpark](https://github.com/apache/spark/tree/0cf59fcbe3799dd3c4469cbf8cd842d668a76f34/python/pyspark) o√π le code est remarquablement simple √† comprendre nottament les fichiers [cloudpickle.py](https://github.com/apache/spark/blob/0cf59fcbe3799dd3c4469cbf8cd842d668a76f34/python/pyspark/cloudpickle.py) et [java_gateway.py](https://github.com/apache/spark/blob/0cf59fcbe3799dd3c4469cbf8cd842d668a76f34/python/pyspark/java_gateway.py) pour les explication de ce blog
- S\'abonner √† la newsletter et √† la page Instagram du blog pour etre inform√© de toutes nos nouvelles publications üòÅ